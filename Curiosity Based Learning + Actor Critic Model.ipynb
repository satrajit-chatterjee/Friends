{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the constants\n",
    "env_name = \"Acrobot-v1\"\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "stability_const = 0.01\n",
    "max_steps = 1000\n",
    "episodes = 10000\n",
    "gamma = 0.95\n",
    "should_save = True\n",
    "embed_dim = 8\n",
    "curiosity_level = 0.2\n",
    "checkpoint = 20\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Creating the environment\n",
    "env = gym.make(env_name)\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating placeholders for all the variables\n",
    "action_tf = tf.placeholder(dtype = tf.float32, shape = [None, action_space], name = \"Action_Placeholder\")\n",
    "reward_tf = tf.placeholder(dtype = tf.float32, shape = [None, 1], name = \"Reward_Placeholder\")\n",
    "state_tf  = tf.placeholder(dtype = tf.float32, shape = [None, state_space], name = \"State_placeholder\")\n",
    "next_state_tf = tf.placeholder(dtype = tf.float32, shape = [None, state_space], name = \"Next_state_placeholder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(logits, predictions):\n",
    "    term1 = -logits * tf.log(tf.clip_by_value(predictions, clip_value_min = 1e-10, clip_value_max = 1))\n",
    "    term2 = -(1 - logits) * tf.log(tf.clip_by_value(1 - predictions, clip_value_min = 1e-10, clip_value_max = 1))\n",
    "    cross_entropy = tf.reduce_mean(term1 + term2)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the network\n",
    "\n",
    "# Building the embedding layer model\n",
    "actions_tf = Input(shape = (None, action_space))\n",
    "states_tf  = Input(shape = (None, state_space))\n",
    "embedding1 = Input(shape = (None, embed_dim))\n",
    "embedding2 = Input(shape = (None, embed_dim))\n",
    "\n",
    "# Creating the embedder\n",
    "Embed1 = Dense(16, activation = \"relu\")(states_tf)\n",
    "Embed2 = Dense(embed_dim, activation = \"relu\")(Embed1)\n",
    "\n",
    "Embedder = Model(inputs = [states_tf], outputs = [Embed2])\n",
    "\n",
    "# Inverse model\n",
    "A_layer1a = Dense(10, activation = \"relu\")(embedding1)\n",
    "A_layer1b = Dense(10, activation = \"relu\")(embedding2)\n",
    "A_layer2  = concatenate([A_layer1a, A_layer1b])\n",
    "A_preds   = Dense(action_space, activation = \"linear\")(A_layer2)\n",
    "\n",
    "InverseModel = Model(inputs = [embedding1, embedding2], outputs = [A_preds])\n",
    "\n",
    "# Creating the forward model\n",
    "F_layer1a = Dense(10, activation = \"relu\")(actions_tf)\n",
    "F_layer1b = Dense(10, activation = \"relu\")(embedding1)\n",
    "F_layer2  = concatenate([F_layer1a, F_layer1b])\n",
    "F_preds   = Dense(embed_dim, activation = \"linear\")(F_layer2)\n",
    "\n",
    "ForwardModel = Model(inputs = [actions_tf, embedding1], outputs = [F_preds])\n",
    "\n",
    "# Creating sequential model policy prediction\n",
    "AC_A_layer1 = Dense(16, activation = \"relu\")(states_tf)\n",
    "AC_A_layer2 = Dense(8, activation = \"relu\")(AC_A_layer1)\n",
    "AC_A_preds = Dense(action_space, activation = \"softmax\")(AC_A_layer2)\n",
    "\n",
    "#Creating the layers for the q value prediction\n",
    "C_layer3 = Dense(5, activation = \"relu\")(actions_tf)\n",
    "C_layer4 = concatenate([C_layer3, AC_A_layer2])\n",
    "Q_values = Dense(1, activation = \"linear\")(C_layer4)\n",
    "\n",
    "#Creating the layers for the value of the state\n",
    "V_values = Dense(1, activation = \"linear\")(AC_A_layer2)\n",
    "\n",
    "#Creating the advantage function\n",
    "Adv_values = Subtract()([Q_values, V_values])\n",
    "\n",
    "Actor = Model(inputs = [states_tf], outputs = [AC_A_preds])\n",
    "Critic = Model(inputs = [states_tf, actions_tf], outputs = [Adv_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the embedder outputs for the state and next state\n",
    "state_embeds = Embedder([state_tf])\n",
    "next_state_embeds = Embedder([next_state_tf])\n",
    "\n",
    "# Feeding the embedded state to the inverse model and finding the loss\n",
    "action_preds = InverseModel([state_embeds, next_state_embeds])\n",
    "inverse_loss = crossEntropy(action_tf, action_preds)\n",
    "\n",
    "# Feeding the states to the forward model to predict next state\n",
    "next_state_preds = ForwardModel([action_tf, state_embeds])\n",
    "intrinsic_reward = tf.losses.cosine_distance(next_state_embeds, next_state_preds, axis = 1)\n",
    "\n",
    "#Building the actor loss\n",
    "ac_action_preds = Actor([state_tf])\n",
    "advantage_values = Critic([state_tf, action_tf])\n",
    "policy_loss = crossEntropy(action_tf, ac_action_preds) * advantage_values\n",
    "\n",
    "#Building the critic loss\n",
    "action_preds_next = tf.argmax(Actor([next_state_tf]), axis = 1)\n",
    "next_best_action  = tf.one_hot(action_preds_next, action_space)\n",
    "reward_preds_next = Critic([next_state_tf, next_best_action])\n",
    "td_loss = stability_const * (gamma * reward_preds_next + reward_tf - advantage_values)\n",
    "\n",
    "#Finding the total loss and minimizing\n",
    "total_loss = tf.reduce_mean(td_loss + policy_loss + inverse_loss - curiosity_level * intrinsic_reward)\n",
    "train = optimizer.minimize(total_loss)\n",
    "\n",
    "# Creating an operation for training\n",
    "train = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an init operation\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Creating a session and initialzing all variables\n",
    "config = tf.ConfigProto(log_device_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the weights\n",
    "saver_path = './Acrobot_Weights/saved_weights.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#Initialzing all variables\n",
    "#saver.restore(sess, saver_path)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================\n",
      "Average extrinsic reward :  -25.0\n",
      "Average steps  25.0\n",
      "Episode Number  0\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -487.4\n",
      "Average steps  487.5\n",
      "Episode Number  20\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -488.05\n",
      "Average steps  488.2\n",
      "Episode Number  40\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -488.55\n",
      "Average steps  488.75\n",
      "Episode Number  60\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -471.2\n",
      "Average steps  471.5\n",
      "Episode Number  80\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -473.4\n",
      "Average steps  473.75\n",
      "Episode Number  100\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -473.1\n",
      "Average steps  473.45\n",
      "Episode Number  120\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -454.85\n",
      "Average steps  455.4\n",
      "Episode Number  140\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -447.3\n",
      "Average steps  447.7\n",
      "Episode Number  160\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -433.6\n",
      "Average steps  434.1\n",
      "Episode Number  180\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -444.25\n",
      "Average steps  444.75\n",
      "Episode Number  200\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -454.65\n",
      "Average steps  455.15\n",
      "Episode Number  220\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -437.6\n",
      "Average steps  438.2\n",
      "Episode Number  240\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -413.1\n",
      "Average steps  413.9\n",
      "Episode Number  260\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -398.45\n",
      "Average steps  399.05\n",
      "Episode Number  280\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -435.25\n",
      "Average steps  435.8\n",
      "Episode Number  300\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -428.05\n",
      "Average steps  428.75\n",
      "Episode Number  320\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -387.55\n",
      "Average steps  388.4\n",
      "Episode Number  340\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -376.15\n",
      "Average steps  376.85\n",
      "Episode Number  360\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -392.6\n",
      "Average steps  393.3\n",
      "Episode Number  380\n",
      "======================================================================================================\n",
      "======================================================================================================\n",
      "Average extrinsic reward :  -399.9\n",
      "Average steps  400.55\n",
      "Episode Number  400\n",
      "======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Training the agent\n",
    "average_reward = 0\n",
    "average_steps  = 0\n",
    "for episode in range(episodes):\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    state   = env.reset()\n",
    "    prev_states = list()\n",
    "    actions = list()\n",
    "    states  = list()\n",
    "    rewards = list()\n",
    "    while True:\n",
    "        prev_state = state\n",
    "        prev_states.append(state)\n",
    "        actions_dist = sess.run(ac_action_preds, feed_dict = {state_tf: np.array(state).reshape(1, state_space)})\n",
    "        action  = np.random.choice(np.arange(0, action_space), p = actions_dist.ravel())\n",
    "        action_ = np.zeros(action_space)\n",
    "        action_[action] = 1\n",
    "        state, reward_ext, done, _ = env.step(action)\n",
    "        \n",
    "        # Getting the intrinsic reward from curiosity\n",
    "        reward = sess.run(intrinsic_reward, feed_dict = {\n",
    "            state_tf: np.array(prev_state).reshape(1, state_space),\n",
    "            next_state_tf : np.array(state).reshape(1, state_space),\n",
    "            action_tf : action_.reshape(1, action_space) \n",
    "        })\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        #Storing data in experience replay\n",
    "        states.append(state)\n",
    "        actions.append(action_)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        #Incrementing step and reward\n",
    "        steps += 1 \n",
    "        total_reward += reward_ext\n",
    "            \n",
    "        if done or steps > max_steps:\n",
    "            #Training model\n",
    "            states[steps - 1] = np.zeros(state_space)\n",
    "            next_states = np.array(states).reshape(steps, state_space)\n",
    "            states = np.array(prev_states).reshape(steps, state_space)\n",
    "            actions = np.array(actions).reshape(steps, action_space)\n",
    "            rewards = np.array(rewards).reshape(steps, 1)\n",
    "            \n",
    "            sess.run(train, feed_dict = {\n",
    "                state_tf: states,\n",
    "                action_tf: actions,\n",
    "                reward_tf: rewards,\n",
    "                next_state_tf: next_states\n",
    "            })\n",
    "                \n",
    "            break\n",
    "            \n",
    "    #Adding to the average reward and average steps\n",
    "    average_reward += total_reward\n",
    "    average_steps += steps\n",
    "    \n",
    "    if episode % checkpoint == 0:\n",
    "        #Printing stats\n",
    "        print(\"======================================================================================================\")\n",
    "        print(\"Average extrinsic reward : \", average_reward / checkpoint)\n",
    "        print(\"Average steps \", average_steps / checkpoint)\n",
    "        print(\"Episode Number \", episode)\n",
    "        print(\"======================================================================================================\")\n",
    "        if should_save:\n",
    "            saver.save(sess, saver_path)\n",
    "#         if average_steps / checkpoint == 200:\n",
    "#             break\n",
    "\n",
    "        #Resetting the stats\n",
    "        average_reward = 0\n",
    "        average_steps  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the agent\n",
    "test_length    = 20\n",
    "average_reward = 0\n",
    "average_steps  = 0\n",
    "for i in range(test_length):\n",
    "    steps = 0\n",
    "    rewards = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        actions_dist = sess.run(ac_action_preds, feed_dict = {state_tf: np.array(state).reshape(1, state_space)})\n",
    "        action  = np.random.choice(np.arange(0, action_space), p = actions_dist.ravel())\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    average_reward += rewards\n",
    "    average_steps  += steps\n",
    "\n",
    "print(\"======================================================================================================\")\n",
    "print(\"Average steps \", average_steps / test_length)\n",
    "print(\"Average reward \", average_reward / test_length)\n",
    "print(\"======================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
